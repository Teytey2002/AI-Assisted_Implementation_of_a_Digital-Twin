# Digital Twin Calibration (dtcalib)

Ce sous-projet fournit une pipeline **propre, test√©e et maintenable** pour :
- charger des s√©ries temporelles entr√©e/sortie issues d‚Äôexp√©riences,
- simuler un mod√®le param√©trique (digital twin),
- calibrer ses param√®tres via **nonlinear least squares**,
- √©valuer la g√©n√©ralisation via **Leave-One-Experiment-Out Cross-Validation (LOO-CV)**,
- mesurer la qualit√© via des m√©triques (MSE/RMSE/NMSE),
- garantir la stabilit√© via des **tests unitaires**.

> √Ä ce stade, le simulateur Python est un mod√®le 1er ordre pour le circuit **R1‚Äì(C || R2)**.
> L‚Äôint√©gration EcoSimPro (simulation haute fid√©lit√©) viendra ensuite.

---

## üìÅ Structure du projet

```
Digital_Twin_Calibration/
‚îú‚îÄ pyproject.toml
‚îú‚îÄ scripts/
‚îÇ ‚îî‚îÄ run_calibration_cv.py
‚îú‚îÄ src/
‚îÇ ‚îú‚îÄ dtcalib/
‚îÇ ‚îú‚îÄ init.py
‚îÇ ‚îú‚îÄ data.py
‚îÇ ‚îú‚îÄ metrics.py
‚îÇ ‚îú‚îÄ simulation.py
‚îÇ ‚îú‚îÄ calibration.py
‚îÇ ‚îî‚îÄ validation.py
‚îú‚îÄ tests/
‚îÇ ‚îú‚îÄ test_data_loading.py
‚îÇ ‚îú‚îÄ test_metrics.py
‚îÇ ‚îú‚îÄ test_simulation.py
‚îÇ ‚îú‚îÄ test_validation.py
‚îÇ ‚îî‚îÄ test_calibration.py

```

## Installation (mode d√©veloppement)
Depuis `Digital_Twin_Calibration/` :

```bash
pip install -e .
```

## Donn√©es attendues

Les datasets sont des dossiers contenant 1 CSV par exp√©rience.

Chaque CSV doit contenir au minimum les colonnes suivantes (noms exacts par d√©faut) :

```
TIME | Addition_2.s_out.signal[1] (entr√©e) | SensorVoltage_1.v (sortie)
```
Exemples de dossiers existants :

```
data/LP_Dataset_csv_Reference
data/LP_Dataset_csv_C_Modified
```

## Modules dtcalib
### dtcalib/data.py

But : g√©rer les exp√©riences et charger les CSV.

Experiment : dataclass (nom, t, u, y, meta)

ExperimentsDataset : container d‚Äôexp√©riences

ExperimentsDataset.from_csv_folder(folder, ...) : charge un dossier de CSV

### dtcalib/metrics.py

But : calculer des m√©triques entre y_true et y_pred.

Metrics.mse(...)

Metrics.rmse(...)

Metrics.nmse(...) avec garde num√©rique

Metrics.compute(...) ‚Üí MetricsResult(rmse, nmse, mse)

### dtcalib/simulation.py

But : fournir une interface de simulation param√©trique uniforme.

Simulator (classe abstraite) : impose simulate(t, u, theta) -> SimulationResult

SimulationResult : sortie simul√©e y + dictionnaire aux (diagnostics)

Simulateurs disponibles :

ExampleRCCircuitSimulator : mod√®le RC 1er ordre (template / debug)

LowPassR1CR2Simulator : mod√®le du circuit r√©el R1‚Äì(C || R2), discretisation exacte ZOH

Circuit Low-pass utilis√© :

u -- R1 -- v
         |-- C -- GND
         |-- R2 -- GND


Ce simulateur permet d‚Äôestimer directement C (ou tau selon config).

### dtcalib/calibration.py

But : calibration des param√®tres par nonlinear least squares.

LeastSquaresCalibrator :

minimise la somme des erreurs au carr√© sur toutes les exp√©riences

supporte bounds, weights, loss robuste (linear, huber, etc.)

CalibrationReport :

theta_hat, cost, success, message, nfev

per_experiment_metrics pour diagnostiquer chaque exp√©rience

### dtcalib/validation.py

But : √©valuer la g√©n√©ralisation via Leave-One-Experiment-Out CV.

LeaveOneExperimentOutCV :

pour chaque exp√©rience, on calibre sur les autres et on teste sur celle-l√†

CrossValidationResult.summary() :

moyenne/√©cart-type des RMSE et NMSE

## Scripts (ex√©cution)
### scripts/run_calibration_cv.py

But : ex√©cuter la pipeline compl√®te sur un dataset r√©el :

chargement dataset

calibration + LOO-CV

affichage summary et quelques folds

(optionnel) affichage stats u/y

Ex√©cution :

python3 scripts/run_calibration_cv.py


√Ä adapter dans le script :

data_folder = Path("...")

choix du simulateur + param√®tres (R1, R2, etc.)

theta0, bounds, max_nfev

## Tests unitaires

Lancer tous les tests :
```
pytest -q
```

### Tests inclus :

test_data_loading.py : v√©rifie le chargement CSV et la validation des colonnes

test_metrics.py : v√©rifie MSE/RMSE/NMSE et erreurs de shape

test_simulation.py : v√©rifie que le simulateur respecte le contrat (shapes, erreurs)

test_validation.py : v√©rifie la logique LOO-CV (avec mock calibrator/simulator)

test_calibration.py : v√©rifie que la calibration retrouve un param√®tre connu (dataset synth√©tique)

Notes de performance

LOO-CV sur 100 exp√©riences = 100 calibrations.

Chaque calibration appelle le simulateur de nombreuses fois (least squares).

Ajouter une progress bar via tqdm est recommand√© pour suivre l‚Äôavancement.


## Auteurs / Contexte

Travail r√©alis√© dans le cadre d‚Äôun TFE (calibration de param√®tres d‚Äôun digital twin) avec datasets g√©n√©r√©s par EcoSimPro.


# üîß Installation du projet
## 1Ô∏è‚É£ Cloner le d√©p√¥t
```
git clone <URL_DU_REPO>
cd Digital_Twin_Calibration
```
## 2Ô∏è‚É£ Cr√©er les environnements Conda

Le projet utilise deux environnements :
- un environnement principal pour le pipeline et les scripts
- un environnement d√©di√© √† l‚Äôentra√Ænement Deep Learning (GPU)

‚ûú Environnement principal
```
conda env create -f environment.yml
conda activate DT_AI
```
‚ûú Environnement Deep Learning (GPU)
```
conda env create -f env_deep_learning.yml
conda activate torch_gpu
```
‚ö†Ô∏è L‚Äôenvironnement torch_gpu est requis pour l‚Äôentra√Ænement sur GPU.

## 3Ô∏è‚É£ Installer le package du projet

Depuis la racine du projet :
```
pip install -e .
```
Cette commande installe le projet en mode d√©veloppement (editable), permettant d‚Äôutiliser le package dtcalib tout en refl√©tant imm√©diatement les modifications du code.

## ‚úÖ V√©rification rapide

Tu peux v√©rifier que l‚Äôinstallation est correcte :
```
python -c "import dtcalib; print(dtcalib.__file__)"
```
